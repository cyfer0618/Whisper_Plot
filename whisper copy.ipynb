{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11ee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "from whisper.tokenizer import Tokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import speechbrain as sb\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import losses\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import torchaudio.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa78061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36c7ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762a196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'joint_ce-0.5accent'\n",
    "# Dict for accents to index.\n",
    "accents_to_index = {\n",
    "    'ABA' : 0, # Arabic\n",
    "    'SKA' : 0,\n",
    "    'YBAA' : 0,\n",
    "#     'ZHAA' : 0,\n",
    "    'BWC' : 1, # Mandarin\n",
    "    'LXC' : 1,\n",
    "    'NCC' : 1,\n",
    "#     'TXHC' : 1,\n",
    "    'ASI' : 2, # Hindi\n",
    "    'RRBI' : 2,\n",
    "    'SVBI' : 2,\n",
    "#     'TNI' : 2,\n",
    "    'HJK' : 3, # Korean\n",
    "    'HKK' : 3,\n",
    "    'YDCK' : 3,\n",
    "#     'YKWK' : 3,\n",
    "    'EBVS' : 4, # Spanish\n",
    "    'ERMS' : 4,\n",
    "    'MBMPS' : 4,\n",
    "#     'NJS' : 4,\n",
    "    'HQTV' : 5, # Vitenamese\n",
    "    'PNV' : 5,\n",
    "    'THV' : 5,\n",
    "#     'TLV' : 5\n",
    "    }\n",
    "\n",
    "held_out_set = {\n",
    "    'ZHAA' : 0,\n",
    "    'TXHC' : 1,\n",
    "    'TNI' : 2,\n",
    "    'YKWK' : 3,\n",
    "    'NJS' : 4,\n",
    "    'TLV' : 5\n",
    "}\n",
    "# Dict for index to accents.\n",
    "index_to_accents = {\n",
    "    0 : 'Arabic',\n",
    "    1 : 'Mandarin',\n",
    "    2 : 'Hindi',\n",
    "    3 : 'Korean',\n",
    "    4 : 'Spanish',\n",
    "    5 : 'Vitenamese'\n",
    "    }\n",
    "\n",
    "PWD = os.getcwd()\n",
    "WHISPER_HIDDEN_LAYER = 384 # Dim of Whisper Last Hidden layer of Encoder\n",
    "TSNE_SAMPLES = 2400 # Number of Data points required to plot T-SNE \n",
    "batch_size = 8 # Batch Size\n",
    "SAMPLING_RATE = 16000 # Required SR for Whisper Feature Extractor\n",
    "TEMPERATURE = 0.1 # Supervised Constrative Loss Temparture\n",
    "ALPHA = 1 # Weight of SCL \n",
    "MODEL = \"tiny.en\" # Pre-Trained Whisper Model\n",
    "NUM_EPOCH = 5\n",
    "MULTILINGUAL = None # For large-v2, please change it to \".en\"\n",
    "PATIENCE = 20 # early stopping patience; how long to wait after last time validation loss improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b3dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{PWD}/dataset/l2_arctic/ZHAA/transcript/'\n",
    "classes = os.listdir(path)\n",
    "classes = [x.split('.')[0] for x in classes]\n",
    "classes_to_index = {}\n",
    "index_to_classes = {}\n",
    "for i, j in enumerate(classes):\n",
    "    classes_to_index[j] = i\n",
    "    index_to_classes[i] = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1d4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d184e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_01_range(x):\n",
    "    value_range = (np.max(x) - np.min(x))\n",
    "    starts_from_zero = x - np.min(x)\n",
    " \n",
    "    return starts_from_zero / value_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27287041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, feature_vectors, labels):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(feature_vectors.squeeze(), p=2, dim=1)\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature, \n",
    "        )\n",
    "        return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a825b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "woptions = whisper.DecodingOptions()\n",
    "wtokenizer = whisper.tokenizer.get_tokenizer(\n",
    "    MULTILINGUAL,\n",
    "    task=woptions.task\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4258bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2ArcticDataset(Dataset):\n",
    "    def __init__(self, split = 'train'):\n",
    "        self.paths = []\n",
    "        self.tokenizer = wtokenizer\n",
    "\n",
    "        # Get paths of the wavs for all accents.\n",
    "        for accent in accents_to_index:\n",
    "            path = f'{PWD}/dataset/l2_arctic/{accent}/wav/'\n",
    "            dir_list = os.listdir(path)\n",
    "            dir_list = [[path + dir, accents_to_index[accent], classes_to_index[dir.split('.')[0]]] for dir in dir_list]\n",
    "            self.paths.extend(dir_list)\n",
    "        \n",
    "        # 80% of the paths \n",
    "        if split == 'train':\n",
    "            self.paths, _ = train_test_split(\n",
    "              self.paths,\n",
    "              shuffle=True,\n",
    "              random_state = 42,\n",
    "              test_size = 0.2\n",
    "            )\n",
    "        # 20% of the paths\n",
    "        elif split == 'val':\n",
    "            _, self.paths = train_test_split(\n",
    "              self.paths,\n",
    "              shuffle=True,\n",
    "              random_state = 42,\n",
    "              test_size = 0.2\n",
    "            )\n",
    "        elif split == 'held':\n",
    "            # Get paths of the wavs for all accents.\n",
    "            self.paths = []\n",
    "            for accent in held_out_set:\n",
    "                path = f'{PWD}/dataset/l2_arctic/{accent}/wav/'\n",
    "                dir_list = os.listdir(path)\n",
    "                dir_list = [[path + dir, held_out_set[accent], classes_to_index[dir.split('.')[0]]] for dir in dir_list]\n",
    "                self.paths.extend(dir_list)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load audio and pad/trim it to fit 30 seconds\n",
    "        audio = whisper.load_audio(self.paths[idx][0])\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "        # make log-Mel spectrogram and move to the same device as the model\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "\n",
    "        accent = self.paths[idx][1]\n",
    "        ngram = self.paths[idx][2]\n",
    "        \n",
    "        transcript = self.paths[idx][0].replace(\"/wav/\", \"/transcript/\")\n",
    "        transcript = transcript.replace(\".wav\", \".txt\")\n",
    "        transcript = open(transcript, 'r').read()\n",
    "        text = [*self.tokenizer.sot_sequence_including_notimestamps] + self.tokenizer.encode(transcript)\n",
    "        labels = text[1:] + [self.tokenizer.eot]\n",
    "\n",
    "        # Return extracted input feature and respective accent as label.\n",
    "        return {\n",
    "            \"input_ids\": mel,\n",
    "            \"accent\": accent,\n",
    "            \"ngram\" : ngram,\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de77624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train, val dataset\n",
    "train_dataset = L2ArcticDataset('train')\n",
    "val_dataset = L2ArcticDataset('val')\n",
    "held_out_dataset = L2ArcticDataset('held')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d78fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperDataCollatorWhithPadding:\n",
    "    def __call__(sefl, features):\n",
    "        mel_augment = transforms.FrequencyMasking(freq_mask_param=80)\n",
    "        input_ids, labels, dec_input_ids, accents, ngrams = [], [], [], [], []\n",
    "        for f in features:\n",
    "            input_ids.append(f[\"input_ids\"])\n",
    "            labels.append(f[\"labels\"])\n",
    "            dec_input_ids.append(f[\"dec_input_ids\"])\n",
    "            accents.append(f[\"accent\"])\n",
    "            ngrams.append(f[\"ngram\"])\n",
    "\n",
    "        for f in features:\n",
    "            input_ids.append(mel_augment(f[\"input_ids\"]))\n",
    "            labels.append(f[\"labels\"])\n",
    "            dec_input_ids.append(f[\"dec_input_ids\"])\n",
    "            accents.append(f[\"accent\"])\n",
    "            ngrams.append(f[\"ngram\"])\n",
    "\n",
    "        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])\n",
    "\n",
    "        \n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        dec_input_ids_length = [len(e) for e in dec_input_ids]\n",
    "        max_label_len = max(label_lengths + dec_input_ids_length)\n",
    "\n",
    "        labels = [\n",
    "            np.pad(\n",
    "                lab,\n",
    "                (0, max_label_len - lab_len),\n",
    "                'constant',\n",
    "                constant_values=-100\n",
    "            )\n",
    "            for lab, lab_len in zip(labels, label_lengths)\n",
    "        ]\n",
    "        \n",
    "        dec_input_ids = [\n",
    "            np.pad(\n",
    "                e,\n",
    "                (0, max_label_len - e_len),\n",
    "                'constant',\n",
    "                constant_values=wtokenizer.eot\n",
    "            ) \n",
    "            for e, e_len in zip(dec_input_ids, dec_input_ids_length)\n",
    "        ]\n",
    "\n",
    "        batch = {\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": dec_input_ids\n",
    "        }\n",
    "\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"accents\"] = torch.tensor(accents, dtype=torch.int64)\n",
    "        batch[\"ngrams\"] = torch.tensor(ngrams, dtype=torch.int64)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad18fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda:1\":\n",
    "  num_workers = 4 # Dataloader stuck after 1 epoch, so have to make it zero.\n",
    "  pin_memory = True\n",
    "else:\n",
    "  num_workers = 0\n",
    "  pin_memory = False\n",
    "\n",
    "# Dataloader for training step.\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=WhisperDataCollatorWhithPadding(),\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "# Dataloader for training step.\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=WhisperDataCollatorWhithPadding(),\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "# Dataloader for Held Out Dataset.\n",
    "held_out_loader = DataLoader(\n",
    "    held_out_dataset, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=WhisperDataCollatorWhithPadding(),\n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "748db978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "set_seed()\n",
    "\n",
    "# Load whsiper model\n",
    "model = whisper.load_model(MODEL)\n",
    "\n",
    "# See extra device is available, if so make data parallel.\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "# Model parameters to device \n",
    "model.to(device)\n",
    "\n",
    "# Compute gradiant for parameters of both encoder and FFN. \n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=2e-6\n",
    "    )\n",
    "\n",
    "# Define Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scloss_fn = losses.SupConLoss(temperature=TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "232c7b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2008 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2008/2008 [32:29<00:00,  1.03batch/s, loss=-7.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss=-6.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [08:14<00:00,  1.01batch/s, loss=-6.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, validation loss=-7.3586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 849/849 [13:46<00:00,  1.03batch/s, loss=1.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, held out loss=0.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2008/2008 [32:26<00:00,  1.03batch/s, loss=-6.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss=-7.4499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [08:11<00:00,  1.02batch/s, loss=-7.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, validation loss=-7.6147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 849/849 [13:44<00:00,  1.03batch/s, loss=1.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, held out loss=0.6203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2008/2008 [32:21<00:00,  1.03batch/s, loss=-9.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train loss=-7.6441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [08:10<00:00,  1.02batch/s, loss=-7.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, validation loss=-7.7480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 849/849 [13:38<00:00,  1.04batch/s, loss=0.434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, held out loss=0.4893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2008/2008 [32:29<00:00,  1.03batch/s, loss=-7.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train loss=-7.7882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [08:10<00:00,  1.02batch/s, loss=-8.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, validation loss=-7.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 849/849 [13:43<00:00,  1.03batch/s, loss=0.501] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, held out loss=0.4081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2008/2008 [32:22<00:00,  1.03batch/s, loss=-8.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, train loss=-7.8821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [08:10<00:00,  1.02batch/s, loss=-8.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, validation loss=-7.8643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 849/849 [13:42<00:00,  1.03batch/s, loss=0.328] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, held out loss=0.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Empty cache for proper utilization of GPU.\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_epochs = NUM_EPOCH\n",
    "training_epoch_loss = []\n",
    "validation_epoch_loss = []\n",
    "held_out_epoch_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(\n",
    "        train_loader,\n",
    "        total=round(len(train_dataset) / batch_size + 0.5),\n",
    "        unit = \"batch\") as tepoch:\n",
    "    \n",
    "        # Training Step\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        total_loss = []\n",
    "        \n",
    "        for batch in tepoch:\n",
    "\n",
    "            # Input_ids --> wav, Accents --> Resp. accent\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            accents = batch[\"accents\"].to(device)\n",
    "            ngrams = batch[\"ngrams\"].to(device)\n",
    "            labels = batch[\"labels\"].long().to(device)\n",
    "            dec_input_ids = batch[\"dec_input_ids\"].long().to(device)\n",
    "\n",
    "            \n",
    "            # Description of progress bar\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}\") \n",
    "            \n",
    "            # Extract Last Layer of Encoder\n",
    "            embedding = model.module.encoder(input_ids)\n",
    "            \n",
    "            # Aggregate using mean. \n",
    "            embedding_mean = torch.mean(embedding, dim=1)\n",
    "\n",
    "            # Pass Embedding decoder to get logits.\n",
    "            out = model.module.decoder(dec_input_ids, embedding)\n",
    "\n",
    "            # Loss = CE + alpha * SCL\n",
    "            loss = loss_fn(out.view(-1, out.size(-1)), labels.view(-1)) - 0.5*(scloss_fn(embedding_mean, accents))\n",
    "\n",
    "            # update loss in progress bar \n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            # Back Prop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss.append(loss.item())\n",
    "        avg_loss = np.array(total_loss).mean()\n",
    "        training_epoch_loss.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, train loss={avg_loss:.4f}\")\n",
    "\n",
    "    with tqdm(\n",
    "        val_loader,\n",
    "        total=round(len(val_dataset) / batch_size + 0.5),\n",
    "        unit = \"batch\") as vepoch:\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        total_loss = []\n",
    "        for batch in vepoch:\n",
    "            \n",
    "            # Input_ids --> .wav, Labels --> Ground Truth, Dec_input_ids --> Decoder's input. \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            accents = batch[\"accents\"].long().to(device)\n",
    "            ngrams = batch[\"ngrams\"].to(device)\n",
    "            labels = batch[\"labels\"].long().to(device)\n",
    "            dec_input_ids = batch[\"dec_input_ids\"].long().to(device)\n",
    "\n",
    "            \n",
    "            # Description of progress bar\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}\") \n",
    "            \n",
    "            # Extract Last Layer of Encoder\n",
    "            embedding = model.module.encoder(input_ids)\n",
    "            \n",
    "            # Aggregate using mean. \n",
    "            embedding_mean = torch.mean(embedding, dim=1)\n",
    "\n",
    "            # Pass Embedding decoder to get logits.\n",
    "            out = model.module.decoder(dec_input_ids, embedding)\n",
    "\n",
    "            # Loss = CE + alpha * SCL\n",
    "            loss = loss_fn(out.view(-1, out.size(-1)), labels.view(-1)) - 0.5*(scloss_fn(embedding_mean, accents))\n",
    "\n",
    "            # update loss in progress bar \n",
    "            vepoch.set_postfix(loss=loss.item())\n",
    "                        \n",
    "            total_loss.append(loss.item())\n",
    "        avg_loss = np.array(total_loss).mean()\n",
    "        validation_epoch_loss.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, validation loss={avg_loss:.4f}\")\n",
    "        \n",
    "    with tqdm(\n",
    "        held_out_loader,\n",
    "        total=round(len(held_out_dataset) / batch_size + 0.5),\n",
    "        unit = \"batch\") as hepoch:\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        total_loss = []\n",
    "        for batch in hepoch:\n",
    "            \n",
    "            # Input_ids --> .wav, Labels --> Ground Truth, Dec_input_ids --> Decoder's input. \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            accents = batch[\"accents\"].long().to(device)\n",
    "            ngrams = batch[\"ngrams\"].to(device)\n",
    "            labels = batch[\"labels\"].long().to(device)\n",
    "            dec_input_ids = batch[\"dec_input_ids\"].long().to(device)\n",
    "\n",
    "            \n",
    "            # Description of progress bar\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}\") \n",
    "            \n",
    "            # Extract Last Layer of Encoder\n",
    "            embedding = model.module.encoder(input_ids)\n",
    "            \n",
    "            # Aggregate using mean. \n",
    "            embedding_mean = torch.mean(embedding, dim=1)\n",
    "\n",
    "            # Pass Embedding decoder to get logits.\n",
    "            out = model.module.decoder(dec_input_ids, embedding)\n",
    "\n",
    "            # Loss = CE + alpha * SCL\n",
    "            loss = loss_fn(out.view(-1, out.size(-1)), labels.view(-1)) - 0.5*(scloss_fn(embedding_mean, accents))\n",
    "\n",
    "            # update loss in progress bar \n",
    "            hepoch.set_postfix(loss=loss.item())\n",
    "                        \n",
    "            total_loss.append(loss.item())\n",
    "        avg_loss = np.array(total_loss).mean()\n",
    "        held_out_epoch_loss.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, held out loss={avg_loss:.4f}\")\n",
    "\n",
    "plt.plot(training_epoch_loss, label='train_loss')\n",
    "plt.plot(validation_epoch_loss, label='val_loss')\n",
    "plt.plot(held_out_epoch_loss, label='held_out_loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{PWD}/loss/\" + f\"loss_joint_{EXP_NAME}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3df3f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(data = 'held_out'):\n",
    "    if data == \"held_out\":\n",
    "        loader = held_out_loader\n",
    "        dataset = held_out_dataset\n",
    "    elif data == \"val\":\n",
    "        loader = val_loader\n",
    "        dataset =  val_dataset\n",
    "\n",
    "    # T-SNE Plot \n",
    "\n",
    "    # Froze batch norm and dropout layer at time of evaluation. \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeds, targets = [], []\n",
    "        with tqdm(\n",
    "            enumerate(loader),\n",
    "            total=round(len(dataset) / batch_size + 0.5),\n",
    "            unit = \"batch\") as tepoch:\n",
    "            for i, batch in tepoch :  \n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                accents = batch[\"accents\"].to(device)\n",
    "                \n",
    "                embeddings = model.module.embed_audio(\n",
    "                    input_ids) \n",
    "\n",
    "                embeddings = torch.mean(embeddings, dim=1)\n",
    "\n",
    "                embeds.extend(embeddings.cpu().detach().numpy())\n",
    "\n",
    "                targets.extend(accents.cpu().detach().numpy())\n",
    "\n",
    "    tsne = TSNE(n_components=2).fit_transform(np.array(embeds))\n",
    "    tx = tsne[:, 0]\n",
    "    ty = tsne[:, 1]\n",
    "\n",
    "    tx = scale_to_01_range(tx)\n",
    "    ty = scale_to_01_range(ty)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"y\"] = [index_to_accents[i] for i in targets]\n",
    "    df[\"comp-1\"] = tx\n",
    "    df[\"comp-2\"] = ty\n",
    "\n",
    "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                    palette=sns.color_palette(\"hls\", len(index_to_accents)),\n",
    "                    data=df).set(title=\"T-SNE projection\")\n",
    "\n",
    "\n",
    "    plt.savefig(f\"{PWD}/t-sne/\" + f\"tnse_{data}_{EXP_NAME}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30e0d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/849 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 849/849 [13:08<00:00,  1.08batch/s]\n",
      "100%|██████████| 502/502 [05:57<00:00,  1.41batch/s]\n"
     ]
    }
   ],
   "source": [
    "plot_tsne('held_out')\n",
    "plot_tsne('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820029fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"{PWD}/models/{EXP_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yashwant",
   "language": "python",
   "name": "yashwant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
